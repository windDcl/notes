## hive调优
> hive用的熟练吗？
> 很熟练，熟练写hive-sql做过一些参数调优

建表：
```
存储格式选择orc或者parquent选择压缩格式snappy

表分区和分桶：分桶是更细粒度的划分，对列分桶，列值的hash%桶数决定在哪个桶里。
分桶的好处：两个表的join字段相同，且都进行了分桶，join性能会大大提升，此时做的是map端join，只在每个桶内进行join即可
```
SQL优化
整体思路是==尽量早的过滤数据，减少job的数量，避免数据倾斜==
```
select 只选择需要的列，尽量不写*
先写子查询再去join表（谓词下推：尽量减少map的输出）
尽量不使用union(合并并去重)而是用 union all 再group by去重
不使用count(distinct)使用 group by
如果需要一张表的字段来约束另一张表，用in比join更高效
子查询尽量不要太多的group by，这样可以减少job的数量

```
参数优化
```
# 有些job比较大，并且有的stage是可能没有依赖关系的。可以让他们并行执行。需要改这个参数并且设置并行度。但是这个只有在资源空闲的时候有效果
hive.exec.parallel=true

# 设置小文件合并相关的参数；包括合并文件的大小，

# 数据倾斜后自动负载均衡
hive.groupby.skewindata=true

# 设置运行引擎为spark

# 大表小表的阈值
```

## hive join的几种方式有哪些
```
left join
right join
join
cross join 笛卡尔积
left semi join  和left join不同是这个只能select左表的字段，并且这个结果返回的是左表和右表的on条件的交集。
```


## 数仓分层的意义


## 遇到过哪些数据倾斜的场景
大表小表通过广播出去，
把key 通过where出来，union  

sql join的区别

## hive-sql
常用的函数
```
# 日期函数
from_unixtime(1237573801,‘yyyy-MM-dd HH:mm:ss’) 时间戳转日期格式
unix_timestamp('2020-01-01 06:06:00’,‘yyyy-MM-dd HH:mm:ss’) 日期转时间戳
datediff(‘2020-02-02’,‘2020-02-01’) =1 日期比较函数
日期加天数 日期减天数
to_date('2020-01-01 06:06:00') = '2020-01-01'

# 字符串相关
substr('2020-01-01', 1,7)=2020-01
split(‘a,b,c,d’,’,’)[0] = a

concat("aa", 11, 2.2)=aa112.2 拼接各种类型
concat_ws("_","aa",string(11),string(2.2))=1. aa_11_2.2 拼接指定分割符
collect_list(字段)将某个字段收集起来，通常和group by结合。分组后将值收集到一个list
collect_set和list类似，只不过元素经过了去重
```


## 数据湖
> 了解数据湖吗？
> 数据中台和数据仓库的区别？
> 离线数仓和传统的存储的区别？

**数据湖的理解**

说白了，就是支持真正的流批一体。具体怎么实现。其实是引入了新的概念：增量消费。
首先解释下时间语义：事件时间和处理时间。正是由于我们处理的大部分是事件时间，而事件时间的数据会有延迟的情况。所以需要划分窗口其实就是分区。

而这些迟到的数据。在流失处理语境下，当然可以实时滚动计算，但是针对离线数据。如果想保证数据一致性，就不得不更新原来的分区数据。

而现有的hadoop存储不支持更新。hudi的增量写入解决了这个问题。从而实现了离线数据和实时数据的真正低延迟、真正的流批一体。

具体做法是：通过记录变更日志，他可以快速定位哪些数据需要更改。

数据湖的发展：
**（1）第一个阶段是数据库**
不管是从业务的角度还是从技术栈角度，大家对数据库都是最熟的。
**（2）第二阶段是数据仓库**
当数据库整体能力达不到我们的存储要求和计算要求后，就出现了数据仓库，定位是偏olap，此时把数据的==存储能力和计算能力通过分布式==的方式来扩大。
**（3）第三阶段是数据湖**
数据湖在存储规模和计算能力进一步扩大，集群规模也变大。整体能力有更多提升，并且具备==实时计算==、机器学习。
**（4）lakehouse**
湖仓一体，由于数据湖有更多事务上的需求，流数据和批数据共享，交互查询能力需要提升。这一阶段能够实现==流批一体==。
具体来说：一张表可以实时读或者批量度或者实时写和批量写。做到了批量数据和实时数据统一存储。在一些情况甚至代码都是一体化的。比如flink-sql。流处理和批处理代码部分是一样的。仅仅可以通过更改参数就可以转变计算模式

其他流批一体：
lambda架构：批量数据和实时是存储和计算分离的。本质流和批没有真正做到统一，实时数据需要同步到离线数据内从而保证数据一致性。
olap架构：其实就是lambda的加强，他可以实现大量数据的实时写入，但是实时和离线之间仍然是异步的。没有真正做到统一。

## flink相关问题
==按照重要程度排序==
### flink的并行度、task、subtask、算子链、slot概念

1. 什么是task、subtask？
> task只是一个类，是一个封装。 每个task可以只封装一个算子，也可封装一个算子链。(为啥要生成算子链？因为如果每个task只封装一个算子，而task可能不在同一个机器，那么即使数据是one-to-one，也会有网络传输的问题，所以flink尽量让一个算子链在一个task内。怎么生成算子链，见问题3。)task还封装了数据的来源和数据的输出在哪。
> task只是一个模板，真正执行会根据并行度创建多个task的实例：也叫subtask。每个subtask就是一个线程。每个subtask的计算逻辑是相同的，只是数据不同。
![[Pasted image 20230721222159.png]]
例如上图，共有8个subtask。2个task。

1. slot（槽位）和并行度的关系
下图，op是算子，Task1有3个并行度，对应有3个subtask，需要分别占用不同的槽位，Task2的subtask可以和task1的subtask共享一个槽位，
![[Pasted image 20230721175322.png]]
> 一个slot可以理解就是一个taskmanager的一个线程单位。
> ***整个集群slot的数量必须大于程序里所有算子中最大的并行度***。因为算子有多少个并行度，这个算子对应的task就会有多少个subtask（task的运行实例），而subtask是真正运行的实例，slot相当于被分配的线程数量，如果被分配的资源不够，subtask就无法执行，这个任务就会失败。
> ***同一个slot可以运行不同task的subtask***，因为不同task的subtask没有并行关系。可以先后执行。运行结束第一个task后，接着可以运行后面的task
> subtask在哪台机器被运行无所谓，subtask的运行位置就是一个slot，**slot是flink的程序调度的最小单位**。
> ![[Pasted image 20230721222159.png]]
> 还看这个图，这个有两个task，8个subtask。至少需要4个slot，因为不同task的subtask可以共享槽位。

3. 算子链的切割
>  三种情况可以形成算子链(或者说槽位共享/算子可以在同一个slot里运行)
>  1. 上下游数据传输one-to-one。
>  2. 上下游并行度相同
>  3. 在同一个共享槽组内（用户可以设置每个算子的共享槽）

### 分区概念
上算子实例计算后的数据，需要发给下游的算子实例，怎么发的问题就是分区。
上下游算子实例之间是有网络连接的，这个连接称为“channel”。上游算子的各个实例怎么分发给下游？默认是。也可以自己指定上游算子分发给下游的分发规则也叫channelSelector。
几种StreamSelector
forward()：上游的所有算子的实例一对一发给下游的实例。上下游并行度必须相同
global：上游的所有算子实例都发给下游te一个task
broadcast：上游的每一个算子实例都要分别下游算子的所有实例，多对多
shuffle：上游算子数据随机分发。
reblacnce：轮循
recale：
partitionCustom：传一个自定义的分区器。按照自己的分区器
keyby(KeySelector)：根据数据的key的hashCode进行hash分发。类似spark里的hashpartitioner。

### flink代码怎样如何分布式运行的
1. 客户端根据代码，首先生成StreamGraph，然后判断是否能chain的条件，将可以chain起来的“点”，chain成operatorChain，得到==jobGraph==
2. 然后提交jobGraph给集群里的jobmanager。
3. jobmanager收到后，将jobmanager再转成==ExecutionGraph==（完美体现各个点的并行度）
4. 最后根据图，来申请所需要的slot槽位，并发送对应的task。


### flink的任务怎么提交

flink可以在standalone集群提交，可以在yarn、k8s，docker上提交
yarn提供的是容器，是最轻量级的，它里面只能运行进程。
k8s(docker)提供的是虚拟机，轻量级的虚拟机，它里面能够运行简化版的操作系统
三种提交模式：
- application mode模式（每个job单独一个集群，job退出，集群退出，main方法在集群运行）---- 1.12版本后有
- per-job模式（每个job单独一个集群，job退出，集群退出，main方法在client运行）
- session模式（多个job共享一个集群：jobmanager/taskmanager，job退出，集群还在，main方法在client运行）
- 这里所谓的flink集群就是一个分布式程序。对于yarn来说就是一个application（抽象的概念）它具体的进程是jobmanager，taskmanager。如果是spark-on-yarn，这个application就是AppMaster和CarseGrainExecutior。

> applicantion模式适合大的job的执行，因为这个每个job之间资源隔离最好，job结束后，集群也就回收了。并且main方法不需要再client执行。对client压力小。session模式适合频发提交小job的情况。因为每次提交任务，不需要重新申请容器。

yarn-session
```
1.申请资源，启动flink集群，注意，这是新版的。此时已经不需要指定taskmanager的数量。此时只是启动了jobmanager，等到提交job的时候会根据并行度再给他提供相应数量的taskmanager。
bin/yarn-session.sh 
-jm 1024 # jobmanager内存
-tm 1024 # taskmanager内存
-s 2 # flink每个taskmanager的槽位
-m yarn-cluster # ：运行到哪里去
-qu yarn队列
2.向集群提交任务
bin/flink run 
-d # 分离模式，否则ctrl-c后就停止了
-yid application_1550579025929_62420 # 申请的yarn的任务id
-p 4 # 本次任务的并行度。提交后，yarn会申请2个taskmanager
-c xxxClass  xxx.jar
```
yarn-perjob
```
# perjob里启动集群和job合二为一，提供每个taskmanager的槽数和job的并行度，同样会自动计算出taskmanager数量。这个例子里taskmanager数量=4/2=2（至少要4个槽，所以4/2=2）
flink run
-m yarn-cluster
-d # 分离模式，否则ctrl-c后就停止了
-yjm 1024 # yarn集群的jobmanager内存
-ytm 1024 
-ys 2
-yqu default
-p 4
-c xxxClass xxx.jar
```

yarn-applicantion-mode
```
# 除了run命令和-m参数不同，其他参数和perjob模式一样
flink run-application 
-t yarn-application # 指定运行模式
-d # 分离模式，否则ctrl-c后就停止了
-yjm 1024 
-ytm 1024
-ys 2
-yqu default
-p 4
-c xxxClass xxx.jar
```

### flink哪些常用算子
#### source类
```
单个并行的：socket
多并行：kafkaSource、readTextFile
自定义source：实现run方法和cancel方法
```

#### 基础transformation算子

映射算子 DataStream->DataStream
```
map:一条变一条 x->y
flatmap：一条变多条。x-> x1, x2,x3,x4
filter: x->true/false
```
分组算子 DataStream->KeyedStream
```
keyby：按照某个字段分组
```
滚动聚合算子
```
sum、min、max、minBy、maxBy这类只能在keyby后使用
这些算子都是来一条数据，对现有的聚合值滚动聚合一次
max和maxBy：区别是max的返回值除了最大值，其他字段都是来的第一条数据。maxby其他字段是最大值的那个字段的那条记录
reduce：需要传入ReduceFunction，自定义一个聚合逻辑
```
#### sink算子
```
1.print
2.StreamFileSink：该sink可以将数据写入各种文件系统，并且可以保证exactly-once还可以对文件分桶存储，支持列式存储格式写入。需要设置文件的生成策略（多长时间写一次，文件多大后生成一个文件）
3.kafka-sink
4.jdbcSink：jdbcSink.sink()这个不保证exactly once。jdbcSink.exactlyOnceSink()可以保证exactlyonce
5.redisSink：maven官网依赖没有，需要自己手动编译放到本地maven仓库
```
#### join算子
```
join需要再窗口中进行操作
{
	// 数据流 1
	SingleOutputStreamOperator<Student> s1;
	// 数据流 2
	SingleOutputStreamOperator<StuInfo> s2;
	// join 两个流，此时并没有具体的计算逻辑
	JoinedStreams<Student, StuInfo> joined = s1.join(s2);
	// 对 join 流进行计算处理
	DataStream<String> stream = joined
	// where 流 1 的某字段 equalTo 流 2 的某字段
	.where(s -> s.getId()).equalTo(s -> s.getId())
	// join 实质上只能在窗口中进行
	.window(TumblingProcessingTimeWindows.of(Time.seconds(20)))
	// 对窗口中满足关联条件的数据进行计算
	.apply(new JoinFunction<Student, StuInfo, String>() {
	// 这边传入的两个流的两条数据，是能够满足关联条件的
	@Override
	public String join(Student first, StuInfo second) throws Exception {
	// 计算逻辑
	// 返回结果
	return
	}
});
```
#### 测流输出
```
通常用来把一个流里不要的数据、或者丢失的数据输出到测流。
```
#### connect
```
这个最大的好处就是两个流可以共享状态。
```

### flink使用过程中遇到的问题
参数问题
```
```
优化
```

```

### flink水位线怎么理解
**问题**：在事件时间的语义下，时间是通过事件的时间来推进的，但是事件携带的时间不能像处理时间一样恒速的推进，同一个分区的时间甚至会乱序。并且上游有多个分区的时间，到下游应该以哪个分区的为准判断数据是否被过滤？需要有一个标准，这个标准就是水位线

水位线是什么？针对事件处理时间语义下，用来单调递增的向前推进时间的一个时间戳标记。如果超过这个标记的数据就不会被处理。

具体怎么实现的？

1. 首先他是==周期性==的产生时间戳，然后将时间戳从源头数据开始向下游传递，==推进整个算子==的时间
2. 从源头开始，没有数据之前，时间戳默认是一个负的最大值
3. ==每个分区==来了第一条数据后，和这个值比较。两者取==最大==更新为新的水位线。这样可以保证分区内数据的有序
4. 上游分发到下游的时候，==分区之间==，需要比较每个分区的水位线取==最小==。这样保证全局有序
5. 由于分区内的数据可能会有延迟，会造成迟到的数据。所以引入延迟时间的参数。当事件时间小于水位线-延迟时间后才会被丢弃
6. 但是水位线通常和窗口结合，用于计算某个时间范围的统计值，如果某个分区的数据一直没有更新，那么在到下游的时候，水位线会不变，然后其他高于这个分区水位线的数据会一直累积着。所以还需要有一个==idle参数，用来设置分区多久没有来新数据==，就会在计算水位线的时候就会跳过该分区。

水位线怎么用？
在数据的源头，为流分配水位线，一种是没有设置延迟时间的，这种完全不容忍分区内的数据乱序。还有一种设置延迟时间，可以容忍一定程度的乱序。

### flink有没有看过一些源码
看过waterMark底层源码
```
source.map(s->bean).assignWatermarkAndTimestamps(WatermarkStrategy.forBoundedOutOfOrderness(Duration.ZERO)).process().print();
```
![[Pasted image 20230723204204.png]]
这个.assignWatermarkAndTimestamps其实也是一个算子，在执行图可以看到算子名称是timestamp/watermark。

forBoundedOutOfOrderness 类可以点进去看一下
![[Pasted image 20230723204852.png]]
点进去
![[Pasted image 20230723204911.png]]
构造方法里把 他的成员变量maxTimeStamp先等于long的最小值，就是一个负的最大值+乱序允许时间+1
这个类还有两个方法
![[Pasted image 20230723205813.png]]
这两个方法我们可以看一下他是在什么情况被调用的，我们在这两个方法都打一下断点，然后debug。
我们可以看到现在还没有数据输入，直接debug到了这里
![[Pasted image 20230723210355.png]]
我们看一下方法调用栈
![[Pasted image 20230723210315.png]]
我们可以看到，首先调用的run方法，这个是Thread调用的，然后调用的是Task的run方法，这个是Flink的Task类，这个类里封装了很多东西，我们知道Task是flink任务的模版。他当然继承了Runnable接口，走到这里并不奇怪。
后面中间都是一些调用，这些比较复杂，能力问题，这里我看不懂，看到后面
![[Pasted image 20230723210830.png]]
我们可以看到这个方法是 TimestampsAndWatermarksOperator 调用的，而这个是一个算子，我们知道Task里是封装了算子的，所以我们着重看一下这个算子。
首先看这个算子的open方法。
![[Pasted image 20230723211604.png]]
这里他的主要工作是初始化watermark生成器，获取了一个水位线周期性发送时间，注册了一个定时器，在这个注册定时器底层我们一直往后面点，最后可以看到注册定时器，当定时器触发的时候，callback方法里调用了onProcessingTime方法
![[Pasted image 20230723211855.png]]

从这里可以看出来，我们的onProcessingTime方法是在TimestampsAndWatermarksOperator的open方法里注册的定时器触发的时候调用的这个方法。

然后我们再回过头看一下onProcessingTime方法
![[Pasted image 20230723212459.png]]
这个方法做了两件事
- 发送水位线
- 载注册一个定时器，当时间到了之后继续触发这个onProcessingTime方法
通过这样就实现了周期性的发送水位线。
周期性发送水位线的方法：
![[Pasted image 20230723212727.png]]
这个方法最后发送的是  最大的时间戳-乱序时间-1

到此我们看完了***onPeriodicEmit***方法的完整过程，总结一下
```
在Task运行的时候，TimestampsAndWatermarksOperator 首先会被初始化执行open方法，初始化了水位线生成器、注册了一个定时器，在200ms(可以配置)后触发，触发后，调用forBoundedOutOfOrderness的周期发送水位线方法，其实就是发送一个时间戳数据给下游。这个时间戳=maxTimestamp-可容忍的乱序时间-1。初始化的maxTimeStamp是long的最小值
```
然后再看下onEvent方法的作用。
debug过程这里省略了，直接说结论
首先它在这里被调用
TimestampsAndWatermarksOperator 算子的方法
![[Pasted image 20230723213612.png]]
来一条数据调用一次这个方法。
onevent方法
![[Pasted image 20230723213707.png]]
这个逻辑就是当前事件时间的时间戳和maxTimeStamp比较，取最大值更新maxTimeStamp。

所以综合上面看的源码，可以理解一下整个过程
- Task运行起来后，初始化TimestampsAndWatermarksOperator的时候就注册了一个定时器，200ms后执行。
	- 会发送一个周期性的时间戳数据(水位线)给下游算子，值为：maxtimeStamp-最迟容忍乱序时间-1；
	- 然后再注册一个定时器。200ms后再次调用这个方法
- 当来了一条数据后，TimestampsAndWatermarksOperator的processElement方法就会更新maxTimeStamp的值，更新逻辑是取事件里的时间和当前的maxTimeStamp比较后取最大。




### flink窗口有哪些
```
滚动窗口
滑动窗口
累计窗口
```

### flink状态怎么理解
状态是什么？

其实就是用来保存==中间结算结果的变量==。比如想对整个流全局累计。那么需要在算子里定义一个变量来记录累计到当前的值。然后到来新的数据的时候将新数据累计到总的变量里。最后输出结果。但是普通的用户==自定义的变量不能容错==，程序运行失败就会丢失。flink的状态实现了容错，失败后重启。flink自己提供了状态管理机制，可以实现故障后的一致性维护，以及状态的高效存储和访问。

状态有哪些？

有两种，一种是算子状态：

1. operatestate。
    
    operatestate一般用来在source的时候使用，
    
    怎么用？
    
    必须实现checkpointedFunction接口。可以重写两个方法：snapshotState（用来在checkpoint之前对状态做一些处理，一般不用谢）和initializeState（主要是传入状态描述器，operatestate只有list类型的state）取数据，需要liststate.get()拿到他。添加使用add。
    
2. keyed-state分成value类型、list类型和map类型，必须要keyby后才能够使用keyedstate，注意这里不是一个分区对应一个state而是==key的hashcode相同==的公用一个state。
    

_**状态怎么保证容错？**_

1. 需要指定checkpoint（设置checkpoint间隔和策略，保存的目录）
    
2. 需要指定task重启策略。默认是某个task失败了这个job就失败，需要设置为fixedDelayRestart需要设置最大失败次数，和重试间隔
    
3. 这些只针对某个task失败重启恢复，但是如果是job级别的失败重启，必须要手动指定从哪里恢复。
    

**_状态的ttl管理有哪些重要参数_**

1. 状态存活时长
    
2. updateTtlOnCreateAndWrite ：在创建和写入一个key对应的状态的时候重置他的存活时间。通常用来解决==热查询==问题
    
3. updateTtlOnReadAndWrite，在读到他的时候就重置存活时间
    
4. setStateVisibility：设置逻辑上已经过期的数据是否还能被查到（一种可以查到，一种不能查到：原理是在查询时计算他的存活时间，如果超时，就过滤，此时不删除，真正删除是后台的线程定时删除的）。默认是查不到的。
    
### flink-sql基本原理和架构
- 首先把数据流绑定对应的schema后，注册成*catalog*里的表，catalog是flink内部表
- 由table-planner利用apache calcite进行**sql语法解析**，绑定元数据后得到逻辑执行计划（语法树）。详细参考[[面试问题#flink-sql基本原理和架构#RBO（基于规则的优化）]]
- 再进行Optimizer进行优化，得到物理执行计划。[[面试问题#flink-sql基本原理和架构#CBO（基于成本的优化）]]
- 物理计划经过代码生成器生成代码，得到transformation tree。因为上面的执行计划只是树，需要转成flink程序的代码树。
- transformation tree转成jobGraph后提交到集群执行。

#### RBO（基于规则的优化）

scan: 读哪些表；filter：where和on后的条件；project：select表达式
![[Pasted image 20230722105107.png]]
常亮折叠：针对select后的每次来一条数据都要计算1+2。直接先算好1+2=3
![[Pasted image 20230722105242.png]]
谓词下推：在filter后的操作可以提前到scan。这样减少数据量
![[Pasted image 20230722105256.png]]
投影下推：select的字段就那些，所以可以在scan的前面做好这件事。只选择对应的字段。最终优化后的逻辑计划是下图。
![[Pasted image 20230722105307.png]]
#### CBO（基于成本的优化）
在得到优化后的逻辑执行计划后，接下来要转成内部的实现方式。比如，join需要选择哪种join（sortmergejoin，hashjoin，boradcasthashjoin），需要根据数据的代价来选择。比如前面的例子里，下推后其中的一个scan数据量变成1千条，那么就直接用boradcasthashjoin。


## spark相关问题
### spark常见的算子
读mysql
```scala
val props = new Properties()  
props.setProperty("user","root")  
props.setProperty("password","root")  
  
val df = spark.read.jdbc("jdbc:mysql://doitedu:3306/realtimedw", "t_md_areas", props)  
```
写出到hive
```scala
resultDF.write  
  .mode(SaveMode.Overwrite)  
  .format("hive")  
  .partitionBy("dt")  
  .saveAsTable("dws.doitedu_mall_app_traffic_aggr_usp")
```
### spark怎么注册udf函数
```scala
spark.udf.register("geo",gps2GeoHashcode)
# 其中gps2GeoHashcode是用户自定义的一个普通函数
```
### spark调优
#### 1.hive on spark执行效率很低有没有什么思路？

从**hive、spark和资源调整**来看这个问题。
1. hive建表合理的指定分区分桶
- 根据常见的查询条件进行分区，比如如果经常按照日期、地区、用户进行查询可以根据这些字段进行分区。
- 避免使用字段里过于唯一的来分区，有可能会导致数据倾斜
- 如果经常查询某个字段可以在分区的基础上进一步分桶，比如经常查询uid和产品id，可以将这个字段进行分桶
2. hive合理设置压缩格式和存储格式

设置hive中间数据压缩功能和最终输出结果的压缩格式。
关于选择哪种压缩算法参考：
```sql
#设置为true为激活中间数据压缩功能，默认是false，没有开启
set hive.exec.compress.intermediate=true;
#设置中间数据的压缩算法
set mapred.map.output.compression.codec= org.apache.hadoop.io.compress.SnappyCodec;

```

观察是否有数据倾斜现象：通常都是这种原因导致的。
缓存数据，观察是否有重复计算的现象，可以通过spark的缓存机制解决。
考虑升级版本，新版本会有很多优化。
硬件资源提升

### spark处理数据倾斜
参考： http://www.jasongj.com/spark/skew/   十分优秀的文章！！！！
如果在读文件的时候有数据倾斜，举例
有10个文件，9个文件都是200M，最后一个8个G。这些文件都没有压缩。
那么这个时候不会有数据倾斜，原因是。spark读文件切分块大小算法是
```scala
protected long computeSplitSize(long goalSize, long minSize, long blockSize) {  
    return Math.max(minSize, Math.min(goalSize, blockSize));  
}
```
minSize就是1B，blockSize如果是linux就和`fs.local.block.size`有关，如果是hdfs就是文件的blocksize有关，默认是128M。==goalSize=所有文件大小/minPartitions。默认是1。==
通过上面的计算公式得到每一块的大小是：max(1B, min((8* 1024+9 * 200)/1,  128))=128，所有每个块都是128M。任务计算时间也相同。

但是如果8G的文件采用的是gzip压缩格式，由于gzip不可切割。这个文件只有20M所以他被单独作为一个task来处理。于是就出现了数据倾斜。

所以在读文件的时候要保证文本文件的==可切割性。==那么哪种文件支持可分割性。参看[[面试问题#压缩算法的选择]]


### spark分区
### spark底层运行原理
### rdd的特性
### spark-sql执行计划
![[Pasted image 20230721105522.png]]
## scala相关问题
### 隐式转换怎么理解
### 半生对象
## yarn的资源调度策略有哪些
- 公平调度
- capacity调度（默认）：同一个队列，各个applicantion使用的是fifo策略，A和B先后进入队列，B需要等A使用完，才能进入
## 线程池的几种创建方式
为什么要使用线程池？

因为频繁的创建线程会消耗很多资源，提前创建好线程，将他们放进容器里，可以减少创建销毁线程的资源使用，在需要使用到线程的时候不需要再去创建，直接使用就好

线程池一共有这几类：

ExecutorService.newFixedThreadPool(3) 参数可以是new Runnable(),那么无返回值，需要调用submit执行这个线程，或者传new Callable，那么Service返回的一个Future类型的值。

底层调用的是ThreadPoolExecutor实现的
```
public ThreadPoolExecutor(int corePoolSize,  
                              int maximumPoolSize,  
                              long keepAliveTime,  
                              TimeUnit unit,  
                              BlockingQueue<Runnable> workQueue,  
                              ThreadFactory threadFactory,  
                              RejectedExecutionHandler handler)
```

- 第一个是核心线程数，它的作用是线程池一开始就会创建这几个线程供别人使用。
- 第二是最大线程数，它的作用是如果核心线程都在被使用且阻塞队列里也排满了，那么就开启最大线程数-核心线程数个线程。
- workQueue就是阻塞队列的类型：
    1. 可以是指定长度的数组类型的阻塞队列，先来先用，它是有界的。
    2. LinkedBlockingQueue是链表类型的阻塞队列，也是先来先用，默认无界，可以设置有界
    3. SynchronousQueue，他不存储元素，只有take操作和put操作，必须take元素后，才能put
- 然后就是拒绝策略：如果最大线程数也满足不了了，那么执行拒绝策略
    1. 直接抛异常
    2. 不抛弃，由调用线程池的线程去执行这个任务
    3. 不抛异常，直接丢弃最早来的任务
    4. 不抛异常，直接丢弃后来的任务
- keepAliveTime的作用是临时创建的线程的空闲存活时间，如果这个临时线程空闲超过这个时间就会被销毁


## 源码阅读技巧：
- 一般当在使用api的时候，如果有些参数拿不定，然后官网又没有详细的解释，在测试的时候也没有得出明确的结论，这个时候就可以看源码，debug，看看它到底什么情况
- 通过debug，查看方法调用栈。
- 想要修改源码，可以将想要修改的那个类，完全复制到当前项目中然后修改。这样运行时会优先运行本项目的类

## Redis相关
### redis缓存击穿和解决方案
redis缓存击穿是：
> 在读redis的时候由于ttl过期，这时大量的请求要读redis的下一层数据库比如mysql，造成mysql崩溃。

解决方法
1. 设置ttl永远不过期（可以另外起一个异步线程判断是否快要过期，在快过期的时候延期）
2. 设置互斥锁
![[Pasted image 20230730230858.png]]
思路是：
> 在redis里设一个锁。key=lock ， value=一个随机数，设置ttl，使用NX模式set这个值（NX的意思是仅在redis的这个key不存在的时候才会赋值成功）。
> 然后如果获取成功，那么就可以操作redis了。在操作redis结束后，释放锁（移除锁的key）
> 如果没有获取成功，就等待重试获取数据。



### redis基本数据结构

## java相关
### hashMap底层原理
参考： https://worktile.com/kb/ask/26327.html
首先hashmap是一种kv键值对类型的数据结构，并且允许null值存在，因为key唯一，所以null也只能有一个。另外他不能保证元素存储的顺序，和放入顺序不一致。

map的put方法：
首先将k，v封装到node对象里。然后调用k的hashcode方法计算出他的code，通过hash算法得出他的数组下标，如果数组该元素没有值，就把这个节点放到这个数组里，如果有链表就遍历链表和每个节点的k进行equals判断，如果有一个返回是true，那么这个node的值就会被覆盖。如果都是false，就把这个新节点放到链表尾部。

map的get方法：
同样先调用k的hashcode方法得到code，根据hash算法得出数组下标，如果这里没有元素，返回null，如果有，遍历链表，比较k的equal方法，如果有一个返回了true，那么返回node的value值，如果都没有，就返回false

hashmap的存储结构是：数组+链表+红黑树。其中存储单元都是k-v的entry对象

### jvm内存模型
参考： https://www.cnblogs.com/mikechenshare/p/16562589.html
![[jvm内存模型.png]]
- 堆
	堆是线程共享的一片内存空间，主要存储具体的对象实例。也是gc回收的主要区域
- 栈空间
	栈属于线程私有的部分，用来存放方法的返回值、操作数栈、本地变量列表、
- 
- 方法区（Method Area）：
    
    方法区属于==线程共享的内存区域==，又称Non-Heap（非堆），主要用于存储已被虚拟机加载的==类信息、常量、静态变量、即时编译器编译后的代码等数据==
    
- JVM堆（Java Heap）：
    
    Java 堆也是属于==线程共享的内存区域==，它在虚拟机启动时创建，是Java 虚拟机所管理的内存中最大的一块，主要用于存放对象实例，几乎所有的对象实例都在这里分配内存，注意Java ==堆是垃圾收集器管理的主要区域==，因此很多时候也被称做GC 堆，如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出OutOfMemoryError 异常。
    
- 程序计数器(Program Counter Register)：
    
    属于==线程私有的数据区域==，是一小块内存空间，主要代表当前线程所执行的字节码行号指示器。字节码解释器工作时，==通过改变这个计数器的值来选取下一条需要执行的字节码指令==，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。
    
- 虚拟机栈(Java Virtual Machine Stacks)：
    
    属于==线程私有的数据区域==，与线程同时创建，总数与线程关联，代表Java方法执行的内存模型。每个方法执行时都会创建一个栈桢来存储方法的的**变量表、操作数栈、动态链接方法、返回值、返回地址**等信息.
    
- 本地方法栈(Native Method Stacks)：
    
    本地方法栈属于线程私有的数据区域，这部分主要与虚拟机用到的 Native 方法相关，一般情况下，我们无需关心此区域。
### 垃圾回收机制


## 压缩算法的选择
参考： https://blog.csdn.net/Shockang/article/details/116424217
为什么要对数据压缩？
>为了优化存储和减少网络带宽


https://blog.csdn.net/xiaoluobutou/article/details/129039788

首先明确存储格式和压缩格式的区别。
存储格式：text、sequence、avro、parquent、orc
压缩格式：snappy、gzip、lzo、lz4、zlib
压缩格式只是对文件存储的一种压缩算法！

下面分析orc和snappy是怎么的一种存储格式，以及snappy压缩到底能不能支持split？
结论先行：
==snappy压缩格式作用在Sequence、Avro、parquet、orc等这些容器类的文件格式上，能够支持切分。但这里的切分并不是因为snappy变的可切分了，而是因为这些容器类的文件格式牛逼==

能切分的意思是：可以从任意位置开始读取文件。而snappy不支持是因为，没有一种索引机制可以确定哪个块对应着
看一下hive表里orc底层存储：
![[Pasted image 20230802184729.png]]
可以看到orc文件从行被切割成一个个的stripe。（带子）
每个stripe的又包含了index data、row data、stripe footer
row data是分列存储的，这一个带子里，每个列是一块以二进制的方式存储。
index data仍然是每个列是一个存储单位，但是记录了列里面每一行具体位置的索引。
而
文件脚注：记录了每个stripe的行数、每列的数据类型、最小值、最大值、求和、行计数等聚合信息

orc-snappy其实压缩的部分只是row data部分，而在读取orc文件的时候只需要两个位置就可以准确的读取操作。
- metadata streams和data stream中每个行组的开始位置。由于每个带子里有很多的行组，想要确定具体的位置，需要读取他的索引信息，但是这个信息并没有被压缩，所以可以直接得到。
- stripes的开始位置，由于一个orc文件由多个stripe组成，每个stripe的开始位置存储在file footer里，这部分也没有被压缩，所以可以快速获取。
所以由于索引信息都没有被压缩，那么就可以快速的得到每一行的具体的位置，
