
## 自我介绍
面试官，你们好，我是敦成亮，今年25岁，从事IT工作4年左右。最近一份工作是在北京久谦咨询技术部门担任大数据开发工程师，今年5月份离职。

工作中我主要负责部分项目的总体架构设计、技术调研、平台集群监控、**数据处理分析代码开发**和一些调优工作。

## 智能营销项目背诵稿


### 1.功能上讲
从业务上讲，本系统主要服务于运营人员使用，帮助运营人员在线自定义和用户行为相关的营销规则。实时监测用户的行为，用户触发规则后，会被推送优惠券、短信或者公众号消息。规则类型包括判断用户行为事件发生次数(可以是多种组合举例子)、用户行为序列、用户画像。规则覆盖的时间范围包括规则上线前后。

事件类型匹配：
> 事件可以说购物车、浏览、收藏
> A事件做了多少次
> A事件多少次并且B事件多少次
> A事件和B事件间隔时间是否超过几分钟

行为序列：
> 在规则上线前到规则上线后的一段时间内先后做了ACE事件。

用户画像：
> 用户是否是男性、是否是水果类爱好者、首单消费金额是否大于xxx

还可以是这些条件的交并差组合。


### 2.技术栈
从技术上讲：主要用的技术是flink+kafka+doris+redis+groovy+enjoy模版

### 3.flink的作用
**flink作为实时计算引擎**，一边消费kafka行为数据、一边消费规则mysql数据。针对到来的每一条行为数据flink判断每种规则是否匹配。

### 4.画像怎么匹配
然后针对不同的规则匹配，有不同的应对方案。并且还要实现匹配逻辑和flink解耦合。
首先针对**用户画像**？整体思路比较简单，其实就是来一条行为数据，就去查询一下这个人的画像，看他能否满足规则里定义的画像条件。

### 5.用户行为匹配计算逻辑
然后就是**用户行为匹配**相关的。比方行为发生次数、行为序列这种。
这种涉及到**时间筛选范围**。
一种情况是：规则里的时间窗口在规则上线以前。
还有一种情况是规则里的时间窗口是在规则上线之后的。
还有一种情况是规则里的时间窗口开始时间在上线以前，结束时间在上线之后

针对第一种这种数据已经成为历史，是离线查询。这种我直接根据规则json拼接sql去doris查询用户的行为明细表就能得到是否匹配。

对于第二种和第三种，我们引入了redis。来存储用户的中间聚合状态。
在得到历史的匹配结果后，将结果先写入redis。针对实时匹配，需要先读初始的聚合状态值是否满足，不满足的话，对中间聚合状态滚动更新，根据这个redis的状态来判断是否已经匹配。
举个例子，假如要看A、B事件间隔是否超过15分钟。在doris中只查询到该用户有A行为。那么在redis里记录
```
ruleId:conditionId   userId->{"A":startTime,"b":null,"isflag":false}
```
上线后，flink程序开始运行。来了某个用户，发现是A事件，就去先查询redis。发现没有匹配，比较两个A的开始时间取最小。后来来了B事件，更新B的最晚时间为B的时间。然后判断两个的差值是否满足规则的规定。如果满足，更新isflag为true。再举例，如果要看A事件是否发生了3次。那么道理是类似的。。。。

### 6.怎么动态加载处理和拼接sql
针对怎样**动态的加载这些处理逻辑**以及拼接sql？
针对不同 的规则需要不同的匹配逻辑，甚至中间存储的数据结构也不同。所以这里必须做解耦。
这里需要用到两个技术：groovy和enjoy

### 7.离线和实时解耦合
然后就是**离线和实时的一个解耦合**
我们的画像系统和查询历史完全是历史行为，完全可以在上线之前搞定。
画像系统：bitmap
历史：也是可以在


## 智能营销的一些问题

### 1.redis 的ttl
> 在存入redis的时候，可以根据rule的开始和结束时间计算出redis的存活时间。

### 2.怎么测试
> 这个其实我们还没有做大量数据的一个数据准确性的校验。只是基于少量数据的各种规则情况的一个人工测试。后续在研究使用jmeter进行压力测试，以及开发自动化测试工具。这个在我走的时候讨论过这个事情。

### 3.怎么提交任务
[[面试问题#flink的任务怎么提交]]

### 4.java开发的框架
> maven最后直接打jar包。

### 5.遇到哪些问题

### 6.你们的界面是什么样的？规则都有哪些

### 7.redis的容量你们有没有观察过。
> 如果数据迁移能够保证一致性吗？

### 8.redis的key有没有什么优化的设计思路

### 9.你们能保证flink写入redis的数据一致性吗？


### 10.数据量
每天100万左右，峰值时间：
10分钟1万条数据
这个数据量并不大，所以三台机器就可以满足。由于提交任务是on-yarn，所以不需要部署flink集群。yarn集群可以说有5台。针对这个数据量完全足够了。


## 实时数仓
这个是客户定制开发的一个项目。针对aldi海外电商做的定制开发项目。主要是做一些实时看板。

需求：两个需求
1. 流量相关的指标展示
2. 订单相关的指标展示

### 1.数据来源
两类：一类是行为事件点击流数据。还有一类是维度相关的表。
两种指标需求：
1. 实时看流量相关指标
2. 实时看订单相关指标

### 2.流量分析
需求：
> 大屏展示：
>计算：截⽌到此刻，==今天==总的pv数，uv数，并且每5分钟更新⼀次结果
>计算：最近10分钟，pv流量、uv数量、会话数量，每1分钟更新一次结果
>计算：每5分钟输出⼀次最近30分钟内的 新⽤⼾数，及其pv数
>计算：每5分钟，访问⼈数最多的前10个⻚⾯及其访问⼈数
>计算：每5分钟，每类⻚⾯中访问⼈数最多的前10个⻚⾯及其访问⼈数

计算逻辑
- 因为分析维度里有用户属性相关，所以需要把用户注册信息表退化到行为日志数据里
- 这段代码使用flink-cdc读取kafka和mysql数据。进行join后输出到kafka-dwd
- 针对上述的几种需求，写flink-sql处理后结果写进mysql
- 另外dwd的数据保留一份在doris里供分析师使用

### 3.订单分析
需求：
主要是一些topn的需求
>截止到此刻，每小时每个品牌中订单额最大的前10种商品及订单额
>每小时更新截止目前的交易额topn

还有一些窗口统计需求
今天累计到当前的订单总额，应付总额，优惠总额（要求每5分钟更新⼀次）

• 今天累计到当前的订单总数，有优惠单数（要求每5分钟更新⼀次）

• 今天各品类累计到当前的订单总额，应付总额，优惠总额（要求每5分钟更新⼀次）

• 今天各品类累计到当前的订单总数，有优惠单数（要求每5分钟更新⼀次）

• 每10分钟更新⼀次最近1⼩时的订单总额

• 每10分钟更新⼀次最近1⼩时的订单总数

• 每10分钟更新⼀次最近1⼩时各品类的订单总额

• 每10分钟更新⼀次最近1⼩时各品类的订单总数

• 今天各⼩时段的订单总额

• 今天各⼩时段的订单总数

• 今天各⼩时段的各品类的订单总额

• 今天各⼩时段的各品类的订单总数

和流量分析类似。需要将订单详情表和商品详情表进行关联得到dwd层。然后写flink-sql讲结果写出到mysql。




数据的特点：
> 商场行为日志数据
> 广告引擎请求日志
> 推荐引擎的请求日志
> 
> 订单表
> 订单商品表
> 用户注册信息表
> 商品信息表
> 页面维度信息表
> 地域信息维度表

为了加速查询效率所以采用了数仓分层。

### 2.数仓分层
ods:
```
用户行为日志原始数据
页面属性维表
设备属性维表
地域属性维表
```

数仓的一个思想：维度退化，将所有分析中需要关联的公共维度提前组合。个性化的维度分主题开发。
维度退化指的是将一些维度表通过用户账号关联到行为明细表中

一些公共维度：
>用户属性维度：会员渠道、注册时间、会员等级、性别、积分。
>页面属性维度：页面标题，页面类型、页面所属的栏目、频道、所属的营销活动等
>地域维度：（行为发生时所在的省市区街道商圈）
>


### 用户属性维度退化
将用户属性表关联到用户行为日志数据中来。
日志数据
```
guid，
eventId，
account，
appid，
deviceId，
ip，
经纬度，
会话id，
渠道号，
产生数据的时间
```

用户注册信息表
```
会员渠道、注册时间、会员等级、性别、积分
```

### 流量主题分析
>需求：
>大屏展示：
>计算：截⽌到此刻，==今天==总的pv数，uv数，并且每5分钟更新⼀次结果
>         使用flink累计窗口函数
>计算：最近10分钟，pv流量、uv数量、会话数量，每1分钟更新一次结果
>计算：每5分钟输出⼀次最近30分钟内的 新⽤⼾数，及其pv数
>计算：每5分钟，访问⼈数最多的前10个⻚⾯及其访问⼈数
>滚动窗口5分钟间隔。
>
>计算：每5分钟，每类⻚⾯中访问⼈数最多的前10个⻚⾯及其访问⼈数

流量分析主题表：dws_traffic_analyse
```
guid,
sessionId,
eventId,
ts, -- 时间戳
pageId,
pageLoadTime,
province,
city,
region,
deviceType,
isNew -- 是否新用户,
会员渠道，
注册时间，
会员等级，
性别，
积分
```
mysql报表数据
```
start_time,
end_time,
page_id,
pv,
uv
```

最近10分钟，pv流量、uv数量、会话数量，每1分钟更新一次结果
[[每10分钟的pv、uv和会话，1分钟更新一次]]]
mysql大屏看板数据
```
url,pageType,pv,uv,updateTime
```

每5分钟输出⼀次最近30分钟内的 新⽤⼾数，及其pv数

```sql
select
  count(distinct uid) as new_cnt,
  count(pageId) as pv
from
table(hop(table dws_traffic, descriptor(t), interval '5' mintues, interval '30' mintues)
	  where isNew=1
	 group by window_start,window_end)
```

每5分钟，访问⼈数最多的前10个⻚⾯及其访问⼈数
```sql
select
  * 
from
(
	select
	  pageId,
	  p_uv,
	  window_start,window_end,
	  row_number() over(partition by pageId,window_start,window_end order by p_uv desc) as rn
	from
	(
		select
		  pageId,
		  count(distinct uid) as p_uv
		from
		table(tumble(table dws_traffic, descriptor(t),interval '5' mintues))
		group by  window_start,window_end,pageId
	) o
)
where rn >=10
```


### 事件概况多维分析
需求：从时间、渠道、事件类型、页面类型。所有的用户交互行为事件
>每10分钟，每个频道，每类事件的  发生次数和发生人数
>每30分钟，每个频道，每类事件的  发生次数和发生人数
>每10分钟，每个频道，每类页面，每类事件，发生的次数和发生人数
>每10分钟，每个频道，每类页面，每类事件，每类终端  发生的次数和发生人数
>
>事件类型举例：
>收藏，加购，下订单，搜索
>频道举例：
>
>页面举例：


这是典型的多维分析，
思路是
1. flink进行最小粒度+userid，轻度聚合写入doris
2. doris创建合适的物化视图加速查询效率


最小粒度：每个用户，每10分钟每类事件、每个频道求  聚合事件的发生次数
物化视图针对此聚合结果，得到各个维度各个事件的发生次数和发生人数
效果是：
可以计算下面的需求：
```
查询 最近 1⼩时内，每10分钟，item_share 事件的⾏为⼈数 和 次数
最近⼀⼩时内，每⼗分钟 item_share事件的发⽣次数和⼈数
当天，每⼩时段search事件发⽣次数最多的⻚⾯类型top3
```

### 订单相关分析
需求：
>截止到此刻，每小时每个品牌中订单额最大的前10种商品及订单额
>每小时更新截止目前的交易额topn

今天截⽌到此刻，每⼩时每个品牌中订单额最⼤的前10种商品及其订单额
```sql
数据源：维度打宽的订单数据。（订单详情表和商品信息表的关联-》存储在mysql）

使用滚动窗口，1小时。使用row_number()
```


spark

spark UI上关注的哪些指标
spark-sql
spark怎么注册udf？

spark里的checkpoint 和rdd的persist区别

flink题：观察截止到当前时间点，看最近一小时股票的最高最低值

重启策略。

checkpoint point











